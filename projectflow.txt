Build a pipeline:
1. Create a github repo and clone it in local(Add experiments).
2. Add src folder along with all the components(run them individually).
3. Add data, models, reports directories to .gitignore file
4. Now git add, commit and push

Setting up dvc pipeline (without params)
5> Create dvc.yaml file and add stages to it.
6> dvc init then do "dvc repro" to test the pipeline automation. (check dvc dag)
7> Now git add, commit, push

Setting up dvc pipeline (with params)
8> add params.yaml file
9> Add the params setup (mentioned below)
10> Do "dvc repro" again to test the pipeline along with the params
11> Now git add, commit, push

Expermients with DVC:
12> pip install dvclive
13> Add the dvclive code block (mentioned below)
14> Do "dvc exp run", it will create a new dvc.yaml(if already not there) and dvclive directory (each run will be considered as an experiment by DVC)
15> Do "dvc exp show" on terminal to see the experiments or use extension on VSCode (install dvc extension)
16> Do "dvc exp remove {exp-name}" to remove exp (optional) | "dvc exp apply {exp-name}" to reproduce prev exp
17> Change params, re-run code (produce new experiments)
18> Now git add, commit, push

Adding a remote s3 storage to DVC:
19> Login to AWS console
20> Create an IAM user
21> Create S3 bucket (enter unique name and create)
22> pip3 install dvc[s3]
23> pip3 install awscli
24> aws configure
25> dvc remote add -d dvcstore s3://bucketname
26> dvc commit-push the exp outcome that you want to keep
27> git add, commit, push